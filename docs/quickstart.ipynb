{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40292af2",
   "metadata": {},
   "source": [
    "# ðŸš€ Quickstart\n",
    "\n",
    "In this quickstart guide, we describe how to use `drlearn` to solve distributionally robust optimization (DRO) problems of the form\n",
    "\n",
    "$$\n",
    "    \\min_{w \\in \\mathbb{R}^d} \\max_{q \\in \\mathcal{Q}} q^\\top \\ell(w) - \\nu D(q \\Vert \\mathbf{1}/n)\n",
    "$$\n",
    "where:\n",
    "- $w$ denotes the parameters of a model (the \"primal variables\"),\n",
    "- $q$ denotes the weights on individual training examples (the \"dual variables\"),\n",
    "- $\\ell: \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$ denotes a loss function for individual training examples,\n",
    "- $D(\\cdot \\Vert \\mathbf{1}/n)$ denotes a divergence (either Kullback-Leibler or $\\chi^2$) between a distribution on $n$ atoms and the uniform distribution $\\mathbf{1}/n = (1/n, \\ldots, 1/n)$,\n",
    "- $\\nu \\geq 0$ is a dual regularization parameter, or the \"shift cost\",\n",
    "\n",
    "The set $\\mathcal{Q}$ is also a hyperparameter of the method, but is often indexed by a single univariate quantity, as we describe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edb178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from drlearn import make_extremile_spectrum, Ridge, BinaryLogisticRegression, MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a467ca6",
   "metadata": {},
   "source": [
    "While one may specify $\\nu$ and the divergence measure, these are set to defaults that generally do not need to change. However, the user might wish to adjust $\\mathcal{Q}$, which is of the form\n",
    "\n",
    "$$\n",
    "    \\mathcal{Q} \\equiv \\mathcal{Q}(\\sigma) = \\operatorname{conv}\\{\\text{permutations of $\\sigma$}\\},\n",
    "$$\n",
    "\n",
    "where $\\sigma = (\\sigma_1, \\ldots, \\sigma_n)$ is a vector of non-negative weights that sums to one, called the *spectrum*. We call $\\mathcal{Q}(\\sigma)$ is the *permutahedron* associated to the vector $\\sigma$. Various choices of $\\sigma$ can be generated by using the `make_<spectrum_name>_spectrum` functions within the package, which return Numpy arrays with value equal to $\\sigma$. See \"Setting Risk Parameters\" in the [documentation site](https://ronakdm.github.io/drlearn/) for more extensive information on spectral risk measures and how to set $\\mathcal{Q}(\\sigma)$. After specifying the spectrum, one can use estimators in the `scikit-learn` interface to fit linear and logit-linear models on data. Examples are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d06ca2",
   "metadata": {},
   "source": [
    "**Regression:** This can be performed using `Ridge`, which mirrors the `Ridge` object from `sklearn.linear_model`. This can be applied to datasets with numerical values, which is a major motivation of distributionally robust methods, as the loss (such as the squared error) can be the ultimate metric of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bc8428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 6)\n",
      "(246,)\n",
      "(62, 6)\n",
      "(62,)\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(name=\"yacht_hydrodynamics\", return_X_y=True, data_home=\"data/\", as_frame=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# prepare data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "center, spread = np.mean(y_train), np.std(y_train)\n",
    "y_train = (y_train - center) / spread\n",
    "y_test = (y_test - center) / spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a514c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X_train.shape\n",
    "spectrum = make_extremile_spectrum(n, 1.5)\n",
    "model1 = Ridge(spectrum=np.ones(shape=(n,)) / n, fit_intercept=False).fit(X_train, y_train)\n",
    "model2 = Ridge(spectrum=spectrum, fit_intercept=False).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade1c5f",
   "metadata": {},
   "source": [
    "`Ridge` objects have standard `predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e42a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.5-quantile of model1: 0.1163\n",
      "Test loss 0.5-quantile of model2: 0.0944\n",
      "Test loss 0.8-quantile of model1: 0.2399\n",
      "Test loss 0.8-quantile of model2: 0.2378\n"
     ]
    }
   ],
   "source": [
    "loss1 = (y_test - model1.predict(X_test))**2\n",
    "loss2 = (y_test - model2.predict(X_test))**2\n",
    "\n",
    "print(f\"Test loss 0.5-quantile of model1: {np.quantile(loss1, 0.5):.4f}\")\n",
    "print(f\"Test loss 0.5-quantile of model2: {np.quantile(loss2, 0.5):.4f}\")\n",
    "\n",
    "print(f\"Test loss 0.8-quantile of model1: {np.quantile(loss1, 0.8):.4f}\")\n",
    "print(f\"Test loss 0.8-quantile of model2: {np.quantile(loss2, 0.8):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b31300",
   "metadata": {},
   "source": [
    "**Classification:** This task can be performed using either the `BinaryLogisticRegression` or the `MultinomialLogisticRegression` objects, depending on whether the problem is framed as a binary or multiclass classification problem. We demonstrate both on a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c57157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 4)\n",
      "(598,)\n",
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(name=\"blood-transfusion-service-center\", version=1, return_X_y=True, data_home=\"data/\", as_frame=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# prepare data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# binary classification with labels in {0, 1}\n",
    "y_train = y_train.astype(int) - 1\n",
    "y_test = y_test.astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5020ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X_train.shape\n",
    "spectrum = make_extremile_spectrum(n, 1.5)\n",
    "model1 = BinaryLogisticRegression(spectrum=spectrum).fit(X_train, y_train)\n",
    "model2 = MultinomialLogisticRegression(spectrum=spectrum).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bf62a",
   "metadata": {},
   "source": [
    "For `BinaryLogisticRegression`, the predicted probabilities will refer to the \"positive\" class (class 1 and *not* class 0). For `MultinomialLogisticRegression`, the return value will be of size `(input_len, n_classes)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2b2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "[0.33138687 0.31341856 0.33675978 0.3160968  0.30987164]\n",
      "(150, 2)\n",
      "[[0.66703463 0.33296537]\n",
      " [0.6989987  0.3010013 ]\n",
      " [0.66149036 0.33850964]\n",
      " [0.69224    0.30776   ]\n",
      " [0.70225582 0.29774418]]\n"
     ]
    }
   ],
   "source": [
    "probas_pred_binary = model1.predict_proba(X_test)\n",
    "probas_pred_multinomial = model2.predict_proba(X_test)\n",
    "\n",
    "print(probas_pred_binary.shape)\n",
    "print(probas_pred_binary[:5])\n",
    "\n",
    "print(probas_pred_multinomial.shape)\n",
    "print(probas_pred_multinomial[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlearn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
